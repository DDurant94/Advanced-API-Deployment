Version: 1.0.1 Advanced E-Commerce API

About:
    In the Advanced E-Commerce API, I have implemented several new features to ensure the secure storage of information, thorough testing of 
  endpoints, comprehensive documentation, and a modular design. This makes it the most in-depth and well-thought-out RESTful API I have built to 
  date. These enhancements allow for readability, maintainability, and scalability over time, even in an ever-changing environment of web security.

    Deployment of this application was done through Render web services, using a Continuous Integration/Continuous Deployment (CI/CD) layout 
  documented within the .github/workflow directory under main.yaml. This setup allows for automated testing and deployment of the application 
  as it scales up in production.
   
   
  Secure Storage and Retrieval:
    Storage of database information is stored withing Renders Postgre API.  

    -utils folder:
      To ensure the safe and secure storage of information, werkzeug.security is implemented for password hashing. When a user signs into 
    their account, werkzeug.security.check_password_hash verifies that the password is correct for the given username. Once both the username 
    and password are verified, a JWT (JSON Web Token) is generated to create a 256-bit hash token for the user. This token is used for accessing 
    different endpoints across the application. The token has a default time limit of one day and one hour, which can be adjusted as needed.

      Within the hash, the user’s role is embedded, allowing access to be granted based on the appropriate role. This approach ensures that only 
    information designated by admins as public can be seen by everyone. As the application grows in complexity, more roles can be created to 
    allow departments to access role-specific areas.


  .GitHub:
    Connecting The CI/CD Pipeline.

    Workflow:
        The main.yaml file contains all the code necessary for building, testing, and deploying the application. It ensures that the virtual 
      environment is correctly set up and that the code is properly tested before the deployment phase. 


  Testing:
    -tests folder:
      For testing various API endpoints, I used unittest and Faker to create mock objects and endpoints. This allowed me to test each endpoint 
    under various constraints, facilitating the refactoring of code for better performance and maintainability under stress. This ensures my 
    code is prepared for all situations, whether it’s handling incorrectly entered data or verifying that the code functions as intended.

      I have used Faker, requests, and unittest to achieve this. Faker generates mock data for use with the API, while requests allow me 
    to interact with the API endpoints to send or retrieve data. Unittest integrates everything, enabling testing without directly 
    inputting data into the database.

      Such testing should be done before building the application to define the data structure before the API is operational. I have 
    set up mock data functions to ensure that data entered into services maintains a consistent structure, serving as a control while 
    working with them. This approach allows each service to handle the same type of data used across all endpoints.

      In conclusion, testing like this is great for validating the performance, security, and reliability of the API and its 
    structure. It allows a coder like myself to identify areas where my code may be lacking and needing improvement, ensuring a 
    better-performing API while isolating each part of the code. If you want to see the tests in action hope over to the test folder and 
    give them a run.


  Documentation:
    -static folder:
      Documentation of data structures, endpoints, and errors is all handled in the swagger.yaml file located in the static folder. Using 
    Swagger and YAML (Yet Another Markup Language), documenting each part of the API becomes much easier and allows for interactive documentation. 
    This gives users a hands-on experience with how information is entered, received, and validated within the API’s code structure.

      Within swagger.yaml, the use of tokens and role verification showcases how security is implemented across the application. To explore the 
    endpoints of this API, open your browser and go to https://advanced-api-deployment.onrender.com/api/docs to interact with all the endpoints 
    and models/schemas.

      The login endpoint has already been set up with the username string and password string, and it has the role of admin to access all the 
    endpoints of this API. After logging in, copy the token, click the lock icon, type Bearer followed by a space, then paste the token and click 
    login. You will now have access to all endpoints.


  Modularized Design:
    E-Commerce uses a modular design to allow for scalability, readability, and ease of debugging.

      Controllers:
          Controllers are where all the data is reviewed using SQLAlchemy to serialize and deserialize data that is given to and received 
      from the database, ensuring the data submitted is in the correct format. When the information doesn’t meet the requirements, 
      implemented error handling lets the user know where they went wrong.

      Models:
            Models lay out each table’s columns, rows, and relationships with other tables to grant access to that information. They also 
        tell the database what kind of information it is looking for, whether it be a string, integer, list, or dictionary. Additionally, 
        they can set restrictions on how long or short the data can be for each row, ensuring that the user enters the correct information.

      Schemas:
          Schemas is a subfolder of Models where the controller checks its data against a schema (model or template) to see if the 
        information matches the columns for that table. 

      Routes:
            Routes specify how the URL should look to find the endpoint for the specified operation you want to perform. Each blueprint can 
        hold several routes, allowing for insertion, selection, deletion, and modification of information within the database.

      Services:
            Services is where all the 'magic' happens. All the data is validated and meticulously reviewed to ensure that when tables share 
        information, it is indeed the correct type of information to be associated with one another. For instance, a product's stock is 
        automatically updated when the day’s production is entered at the end of the day. Data can be parsed and modified within services.  


  Other Notable Features:

    Limiter:
        A limit of 100 requests per day has been implemented on all endpoints. This can be adjusted to reflect the application’s traffic needs. 
      The limit is set at 100 to prevent the API from being overused and to ensure it operates at its best capacity at all times. This also 
      helps prevent any single user from abusing and overloading the API with excessive calls.

    Caching:
        Caching is used to save information in the user’s local memory for 60 seconds by default, helping to prevent the API from being overloaded 
      by numerous GET requests. This reduces stress on the API and helps prevent crashes. Caching has been implemented on all GET requests, 
      and the default time can be adjusted as needed. However, the drawback of caching is that if any changes occur within the set time frame, 
      the cache will not reflect those changes until the endpoint is called again after the timer has expired.

    Pagination:
        Pagination is used to limit the amount of information the API retrieves at one time, ensuring that only a manageable amount is displayed 
      on a single page. By default, this is set to 10 items per page. This approach prevents the API from being overloaded and conserves 
      resources on the user’s end, particularly memory.


Installs:
  Refer to requirements.txt to see all of the installs used. You can also run the command bellow for quick install into your VENV 
  (virtual environment) with versions used.
PIP install Command:
  pip install Flask SQLAlchemy marshmallow flask-marshmallow Flask-SQLAlchemy Flask-Caching mysql-connector-python marshmallow-sqlalchemy circuitbreaker Flask-Limiter flask-cors pyjwt python-dotenv Faker pytest pytest-mock requests flask-swagger flask-swagger-ui psycopg2 psycopg2-binary gunicorn


Questions:
  The e-commerce project, developed in Module 13, requires a streamlined Continuous Integration and Continuous Deployment (CI/CD) pipeline 
  for optimal performance. This pipeline should automate the process of building, testing, and deploying the application to Render, a cloud 
  platform. Key components include setting up a robust CI/CD workflow using GitHub Actions, configuring a PostgreSQL database hosted on Render 
  for the Flask API, and implementing unit tests using unittest and pytest for application reliability. The project will emphasize documentation 
  and knowledge sharing to ensure efficient collaboration and project management.

  Project requirements

    Utilize the e-commerce project generated in Module 13:
      Use the e-commerce project created in Module 13 as the basis for this CI/CD project.

    Send the project to the GitHub repository (if haven’t already):
      Push the e-commerce project to a GitHub repository to facilitate version control and collaboration.

    Implement a Continuous Integration (CI) flow of build and test in GitHub Actions:
      Create a main.yml file within the .github/workflows directory to define the CI workflow.
      Configure the workflow to automatically trigger code pushes to the main branch.
      Use GitHub Actions to build the project and run unit tests using unittest and pytest.
      Ensure that the workflow fails if any tests fail, preventing the deployment of faulty code.

    Deploy a PostgreSQL database on Render and link it with the Flask API:
      Set up a PostgreSQL database service on Render to host the application's database.
      Modify the Flask API code to use the psycopg2 library to connect to the Render-hosted PostgreSQL database.
      Configure the database connection settings in the Flask API to establish a successful connection.

    Implement a Continuous Deployment (CD) flow in GitHub Actions with deployment to Render:
      Extend the existing GitHub Actions workflow to include a deployment stage.
      Define deployment jobs to deploy the application to Render.
      Ensure that the deployment only occurs after the CI tests have passed successfully.

    Validate the API's functionality through Swagger-generated documentation:
      Integrate the Swagger documentation into the Flask API to provide a comprehensive overview of the API's endpoints and functionality.
      Ensure that the Swagger documentation is up-to-date and reflects the current state of the API.
      Use the Swagger UI to interact with the API and verify its functionality, including testing various endpoints and verifying responses.

  GitHub Repository:
    Maintain a clean and interactive README.md file in the GitHub repository, providing clear instructions on how to run the application and 
    explanations of its features.

    Include a link to the GitHub repository in the project documentation.

  Submission
    Upon completing the project, submit your code and video, including all source code files, and the README.md file in your GitHub repository 
    to your instructor or designated platform.

  Project Tips
    Database Connection Configuration:
      Install the psycopg2-binary and psycopg2 libraries within the requirements.txt file to establish a PostgreSQL database connection.
      Ensure that libpq-dev is installed to resolve any potential dependency issues.

    Pipeline Configuration:
      Utilize multiple jobs within the GitHub Actions workflow to achieve a better understanding of the CI/CD process.
      Create a .github/workflows directory and a main.yml file to define the CI/CD workflow.

    Integration Continuous Testing:
      Implement unit tests for the Flask API using the unittest and pytest libraries.
      Ensure that the CI workflow includes a test step that runs these unit tests to validate the functionality of the e-commerce project.

    Deploy Databases in Render:
      Leverage Render's built-in PostgreSQL database service to host the application's database.
      Configure the database connection settings in the Flask API to connect to the Render-hosted PostgreSQL database.
      Install Gunicorn for Production Deployment
      Install the gunicorn library within the requirements.txt file to facilitate the deployment of the Flask API to Render.